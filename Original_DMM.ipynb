{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions import TransformedDistribution\n",
    "from pyro.distributions.transforms import affine_autoregressive\n",
    "from pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO, TraceEnum_ELBO, TraceTMC_ELBO, config_enumerate\n",
    "from pyro.optim import ClippedAdam\n",
    "\n",
    "import mido\n",
    "from mido import Message, MidiFile, MidiTrack, MetaMessage\n",
    "\n",
    "from midi2audio import FluidSynth\n",
    "\n",
    "import pretty_midi\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import wave\n",
    "from scipy.io.wavfile import write\n",
    "import fluidsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## args\n",
    "num_epochs=5000\n",
    "learning_rate=0.003\n",
    "rpt_eps = 1e-20\n",
    "rnn_clamp = 1.\n",
    "# learning_rate=0.0003\n",
    "beta1=0.96\n",
    "beta2=0.999\n",
    "clip_norm=10.0\n",
    "lr_decay=0.99996\n",
    "mini_batch_size=20\n",
    "annealing_epochs=1000\n",
    "minimum_annealing_factor=0.2\n",
    "rnn_dropout_rate=0.1\n",
    "num_iafs=0\n",
    "iaf_dim=100\n",
    "checkpoint_freq=100\n",
    "load_opt=''\n",
    "load_model=''\n",
    "save_opt=''\n",
    "save_model=''\n",
    "cuda=False\n",
    "jit=False\n",
    "tmc=False\n",
    "tmcelbo=False\n",
    "rpt = True\n",
    "tmc_num_samples=10\n",
    "log='dmm.log'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 長さ最長129、例えば長さが60のやつは61~129はすべて0データ\n",
    "data = poly.load_data(poly.JSB_CHORALES)\n",
    "training_seq_lengths = data['train']['sequence_lengths']\n",
    "training_data_sequences = data['train']['sequences']\n",
    "\n",
    "## super easy training set\n",
    "training_seq_lengths = torch.tensor([8])\n",
    "training_data_sequences = torch.zeros(10,8,88)\n",
    "for i in range(5):\n",
    "    for j in range(8):\n",
    "        training_data_sequences[i][j][int(20+i*10)] = 1\n",
    "for i in range(5,8):\n",
    "    training_data_sequences[i][0][int(110-i*10)  ] = 1\n",
    "    training_data_sequences[i][1][int(110-i*10)+2] = 1\n",
    "    training_data_sequences[i][2][int(110-i*10)+2] = 1\n",
    "    training_data_sequences[i][3][int(110-i*10)+1] = 1\n",
    "    training_data_sequences[i][4][int(110-i*10)+2] = 1\n",
    "    training_data_sequences[i][5][int(110-i*10)+2] = 1\n",
    "    training_data_sequences[i][6][int(110-i*10)+2] = 1\n",
    "    training_data_sequences[i][7][int(110-i*10)+1] = 1\n",
    "\n",
    "test_seq_lengths = data['test']['sequence_lengths']\n",
    "test_data_sequences = data['test']['sequences']\n",
    "val_seq_lengths = data['valid']['sequence_lengths']\n",
    "val_data_sequences = data['valid']['sequences']\n",
    "N_train_data = len(training_seq_lengths)\n",
    "N_train_time_slices = float(torch.sum(training_seq_lengths))\n",
    "N_mini_batches = int(N_train_data / mini_batch_size +\n",
    "                    int(N_train_data % mini_batch_size > 0))\n",
    "\n",
    "\n",
    "# how often we do validation/test evaluation during training\n",
    "val_test_frequency = 50\n",
    "# the number of samples we use to do the evaluation\n",
    "n_eval_samples = 1\n",
    "\n",
    "# rep is short for \"repeat\"\n",
    "# which means how many times we use certain sample to do validation/test evaluation during training\n",
    "def rep(x):\n",
    "        rep_shape = torch.Size([x.size(0) * n_eval_samples]) + x.size()[1:]\n",
    "        repeat_dims = [1] * len(x.size())\n",
    "        repeat_dims[0] = n_eval_samples\n",
    "        return x.repeat(repeat_dims).reshape(n_eval_samples, -1).transpose(1, 0).reshape(rep_shape)\n",
    "\n",
    "# get the validation/test data ready for the dmm: pack into sequences, etc.\n",
    "val_seq_lengths = rep(val_seq_lengths)\n",
    "test_seq_lengths = rep(test_seq_lengths)\n",
    "val_batch, val_batch_reversed, val_batch_mask, val_seq_lengths = poly.get_mini_batch(\n",
    "    torch.arange(n_eval_samples * val_data_sequences.shape[0]), rep(val_data_sequences),\n",
    "    val_seq_lengths, cuda=cuda)\n",
    "test_batch, test_batch_reversed, test_batch_mask, test_seq_lengths = poly.get_mini_batch(\n",
    "    torch.arange(n_eval_samples * test_data_sequences.shape[0]), rep(test_data_sequences),\n",
    "    test_seq_lengths, cuda=cuda)"
   ]
  },
  {
   "source": [
    "# Emitter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emitter(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the bernoulli observation likelihood `p(x_t | z_t)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, z_dim, emission_dim):\n",
    "        super().__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z_t):\n",
    "        \"\"\"\n",
    "        Given the latent z at a particular time step t we return the vector of\n",
    "        probabilities `ps` that parameterizes the bernoulli distribution `p(x_t|z_t)`\n",
    "        \"\"\"\n",
    "        h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
    "        ps = torch.sigmoid(self.lin_hidden_to_input(h2))\n",
    "        return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Transmitter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the gaussian latent transition probability `p(z_t | z_{t-1})`\n",
    "    See section 5 in the reference for comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super().__init__()\n",
    "        # initialize the six linear transformations used in the neural network\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
    "        # modify the default initialization of lin_z_to_loc\n",
    "        # so that it's starts out as the identity function\n",
    "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
    "        # initialize the three non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1):\n",
    "        \"\"\"\n",
    "        Given the latent `z_{t-1}` corresponding to the time step t-1\n",
    "        we return the mean and scale vectors that parameterize the\n",
    "        (diagonal) gaussian distribution `p(z_t | z_{t-1})`\n",
    "        \"\"\"\n",
    "        # compute the gating function\n",
    "        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n",
    "        gate = torch.sigmoid(self.lin_gate_hidden_to_z(_gate))\n",
    "        # compute the 'proposed mean'\n",
    "        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n",
    "        # assemble the actual mean used to sample z_t, which mixes a linear transformation\n",
    "        # of z_{t-1} with the proposed mean modulated by the gating function\n",
    "        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean\n",
    "        # compute the scale used to sample z_t, using the proposed mean from\n",
    "        # above as input the softplus ensures that scale is positive\n",
    "        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return loc, scale\n"
   ]
  },
  {
   "source": [
    "# Combiner"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combiner(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes `q(z_t | z_{t-1}, x_{t:T})`, which is the basic building block\n",
    "    of the guide (i.e. the variational distribution). The dependence on `x_{t:T}` is\n",
    "    through the hidden state of the RNN (see the PyTorch module `rnn` below)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super().__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n",
    "        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1, h_rnn):\n",
    "        \"\"\"\n",
    "        Given the latent z at at a particular time step t-1 as well as the hidden\n",
    "        state of the RNN `h(x_{t:T})` we return the mean and scale vectors that\n",
    "        parameterize the (diagonal) gaussian distribution `q(z_t | z_{t-1}, x_{t:T})`\n",
    "        \"\"\"\n",
    "        # combine the rnn hidden state with a transformed version of z_t_1\n",
    "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
    "        # use the combined hidden state to compute the mean used to sample z_t\n",
    "        loc = self.lin_hidden_to_loc(h_combined)\n",
    "        # use the combined hidden state to compute the scale used to sample z_t\n",
    "        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return loc, scale\n"
   ]
  },
  {
   "source": [
    "# Deep Markov Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM(nn.Module):\n",
    "    \"\"\"\n",
    "    This PyTorch Module encapsulates the model as well as the\n",
    "    variational distribution (the guide) for the Deep Markov Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=88, z_dim=100, emission_dim=100,\n",
    "                 transition_dim=200, rnn_dim=600, num_layers=1, rnn_dropout_rate=0.1,\n",
    "                 num_iafs=0, iaf_dim=50, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # instantiate PyTorch modules used in the model and guide below\n",
    "        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n",
    "        self.trans = GatedTransition(z_dim, transition_dim)\n",
    "        self.combiner = Combiner(z_dim, rnn_dim)\n",
    "        # dropout just takes effect on inner layers of rnn\n",
    "        rnn_dropout_rate = 0. if num_layers == 1 else rnn_dropout_rate\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, nonlinearity='relu',\n",
    "                          batch_first=True, bidirectional=False, num_layers=num_layers,\n",
    "                          dropout=rnn_dropout_rate)\n",
    "\n",
    "        # if we're using normalizing flows, instantiate those too\n",
    "        self.iafs = [affine_autoregressive(z_dim, hidden_dims=[iaf_dim]) for _ in range(num_iafs)]\n",
    "        self.iafs_modules = nn.ModuleList(self.iafs)\n",
    "\n",
    "        # define a (trainable) parameters z_0 and z_q_0 that help define the probability\n",
    "        # distributions p(z_1) and q(z_1)\n",
    "        # (since for t = 1 there are no previous latents to condition on)\n",
    "        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        # define a (trainable) parameter for the initial hidden state of the rnn\n",
    "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        # if on gpu cuda-ize all PyTorch (sub)modules\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "\n",
    "        # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "        # to be in contiguous gpu memory\n",
    "        h_0_contig = self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()\n",
    "        # if any(torch.isnan(h_0_contig.reshape(-1))):\n",
    "        #     print(\"h_0_contig\")\n",
    "        # push the observed x's through the rnn;\n",
    "        # rnn_output contains the hidden state at each time step\n",
    "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "        if any(torch.isnan(rnn_output.data.reshape(-1))):\n",
    "            print(\"rnn_output First\")\n",
    "            # print(self.rnn.state_dict().items())\n",
    "            print(rnn_output)\n",
    "            torch.save(rnn_output, \"out\")\n",
    "            torch.save(self.rnn.state_dict().items, \"dic\")\n",
    "            torch.save(mini_batch_reversed, \"mini_batch_reversed\")\n",
    "            torch.save(h_0_contig, \"h_0_contig\")\n",
    "            assert False\n",
    "\n",
    "        # reverse the time-ordering in the hidden state and un-pack it\n",
    "        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "        # if any(torch.isnan(rnn_output.reshape(-1))):\n",
    "        #     print(\"rnn_output\")\n",
    "        # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n",
    "        z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n",
    "        # if any(torch.isnan(z_prev.reshape(-1))):\n",
    "        #     print(\"z_prev\")\n",
    "\n",
    "        x_container = []\n",
    "        for t in range(1,T_max+1):\n",
    "            # the next two lines assemble the distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "            z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "\n",
    "            # Reparameterization Trick\n",
    "            eps = torch.randn(z_loc.size())\n",
    "            z_t = z_loc + z_scale * eps\n",
    "\n",
    "            # compute the probabilities that parameterize the bernoulli likelihood\n",
    "            emission_probs_t = self.emitter(z_t)\n",
    "\n",
    "            # the next statement instructs pyro to observe x_t according to the\n",
    "            # bernoulli distribution p(x_t|z_t)\n",
    "\n",
    "            # No Reparameterization Trick\n",
    "            x = emission_probs_t\n",
    "\n",
    "            #Reparameterization Trick\n",
    "            if rpt : \n",
    "                eps = torch.rand(88)\n",
    "                # assert len(emission_probs_t) == 88\n",
    "                appxm = torch.log(eps + rpt_eps) - torch.log(1-eps + rpt_eps) + torch.log(x + rpt_eps) - torch.log(1-x + rpt_eps)\n",
    "                # appxm = torch.log(eps) - torch.log(1-eps) + torch.log(x) - torch.log(1-x)\n",
    "                x = torch.sigmoid(appxm)\n",
    "\n",
    "                \n",
    "\n",
    "            # the latent sampled at this time step will be conditioned upon\n",
    "            # in the next time step so keep track of it\n",
    "            z_prev = z_t\n",
    "            x_container.append(x)\n",
    "\n",
    "        x_container = torch.stack(x_container)\n",
    "        return x_container.transpose(0,1)\n",
    "                "
   ]
  },
  {
   "source": [
    "# Wasserstein Distance ( WGAN version )"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_network(nn.Module):\n",
    "    def __init__(self, hiddden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        ## the number of tones\n",
    "        self.song_size = 88\n",
    "\n",
    "        self.hidden_size = hiddden_dim\n",
    "        self.D =  nn.Sequential(\n",
    "                    nn.Linear(self.song_size, self.hidden_size),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Linear(self.hidden_size, 1))\n",
    "    \n",
    "    def forward(self, train_mini_batch, generated_mini_batch):\n",
    "        outputs_real = self.D(train_mini_batch)\n",
    "        outputs_fake = self.D(generated_mini_batch)\n",
    "        # print(outputs_real.size())\n",
    "        # print(outputs_real.size())        \n",
    "        # if any(torch.isnan(outputs_real.reshape(-1))):\n",
    "        #     print(\"REAL\")\n",
    "        # if any(torch.isnan(generated_mini_batch.reshape(-1))):\n",
    "        #     print(\"GENERATED\")\n",
    "        # if any(torch.isnan(outputs_fake.reshape(-1))):\n",
    "        #     print(\"FAKE\")\n",
    "        # if torch.isnan(-(torch.mean(outputs_real) - torch.mean(outputs_fake))):\n",
    "        #     print(\"OUTPUT\")\n",
    "        return -(torch.mean(outputs_real) - torch.mean(outputs_fake))\n",
    "\n",
    "\n",
    "\n",
    "class WassersteinLoss():\n",
    "    def __init__(self, WGAN_network, N_loops=5, lr=0.00001):\n",
    "        self.WGAN_network = WGAN_network\n",
    "        self.D = self.WGAN_network.D\n",
    "        # self.optimizer = torch.optim.RMSprop(self.D.parameters(), lr=lr)\n",
    "        self.optimizer = torch.optim.Adam(self.D.parameters(), lr=lr)\n",
    "        # the number of loops of calculation of Wass\n",
    "        self.N_loops = N_loops\n",
    "\n",
    "\n",
    "    def calc(self, train_mini_batch, generated_mini_batch):\n",
    "        # vanish grad_fn of DMM's parameters\n",
    "        no_grad_generated_mini_batch = torch.tensor(generated_mini_batch)\n",
    "\n",
    "        # activate grad_fn of Wass calculator's parameters \n",
    "        for p in self.D.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        ### CALCULATION LOOP ###       \n",
    "        for i in range(self.N_loops):\n",
    "            # print(\"{0}%\".format(i*20))\n",
    "            # NaN_detect(self.D,i,\"Before calc\")\n",
    "            loss = self.WGAN_network(train_mini_batch, no_grad_generated_mini_batch)\n",
    "            # print(loss)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward() # 勾配を計算\n",
    "            self.optimizer.step() # 重みパラメータを更新 (Algorithm 1のStep 6)\n",
    "\n",
    "            # NaN_detect(self.D,i,\"Before Clip\")\n",
    "            # 重みパラメータの値を-0.01から0.01の間にクリッピングする (Algorithm 1のStep 7)\n",
    "            for p in self.D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "            # NaN_detect(self.D,i,\"After Clip\")            \n",
    "\n",
    "        # vanish grad_fn of Wass calculator's parameters\n",
    "        # because we don't need to update Wass calculator's parameters anymore\n",
    "        for p in self.D.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # calculate Wass without Wass calculator's parameters's grad_fn\n",
    "        loss = self.WGAN_network(train_mini_batch, generated_mini_batch)\n",
    "\n",
    "        # save model\n",
    "        # torch.save(self.D.state_dict(), \"D\")\n",
    "\n",
    "        # Wass is positive\n",
    "        return -loss\n"
   ]
  },
  {
   "source": [
    "# Set the DMM and the Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-cd073b2d280c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# generate mini batch from training mini batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# NaN_detect(dmm, epoch, message=\"Before Generate\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgenerated_mini_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_reversed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_seq_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;31m# NaN_detect(dmm, epoch, message=\"After Generate\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# if any(torch.isnan(generated_mini_batch.reshape(-1))):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7f8b2d8ad2fd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths, annealing_factor)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# rnn_output contains the hidden state at each time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mrnn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch_reversed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_0_contig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rnn_output First\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m# print(self.rnn.state_dict().items())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    592\u001b[0m                           \u001b[1;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[1;32m--> 594\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dmm = DMM()\n",
    "WN = WGAN_network()\n",
    "W = WassersteinLoss(WN)\n",
    "\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(dmm.parameters(), lr=learning_rate)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "# make directory for Data\n",
    "now = datetime.datetime.now().strftime('%Y%m%d_%H_%M')\n",
    "os.makedirs(\"saveData\\\\\"+datetime.datetime.now().strftime('%Y%m%d_%H_%M'))\n",
    "\n",
    "num_epochs = 1000\n",
    "checkpoint_freq = 500\n",
    "#######################\n",
    "#### TRAINING LOOP ####\n",
    "#######################\n",
    "times = [time.time()]\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_nll = 0\n",
    "    shuffled_indices = torch.randperm(N_train_data)\n",
    "    # print(\"Proceeding: %.2f \" % (epoch*100/5000) + \"%\")\n",
    "\n",
    "    # process each mini-batch; this is where we take gradient steps\n",
    "    for which_mini_batch in range(N_mini_batches):\n",
    "\n",
    "        # compute which sequences in the training set we should grab\n",
    "        mini_batch_start = (which_mini_batch * mini_batch_size)\n",
    "        mini_batch_end = np.min([(which_mini_batch + 1) * mini_batch_size, N_train_data])\n",
    "        mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n",
    "\n",
    "        # grab a fully prepped mini-batch using the helper function in the data loader\n",
    "        mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n",
    "            = poly.get_mini_batch(mini_batch_indices, training_data_sequences,\n",
    "                                    training_seq_lengths, cuda=cuda)\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # generate mini batch from training mini batch\n",
    "        # NaN_detect(dmm, epoch, message=\"Before Generate\")                          \n",
    "        generated_mini_batch = dmm(mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths)\n",
    "        # NaN_detect(dmm, epoch, message=\"After Generate\")\n",
    "        # if any(torch.isnan(generated_mini_batch.reshape(-1))):\n",
    "        #     print(\"GENERATED\")\n",
    "        #     assert False\n",
    "        # NaN_detect(WN, epoch, message=\"calc_Before\")\n",
    "        # calculate loss\n",
    "        loss = W.calc(test_batch, generated_mini_batch)\n",
    "        # NaN_detect(WN, epoch, message=\"calc_After\")\n",
    "\n",
    "        # NaN_detect(dmm, epoch, message=\"step_Before\")        \n",
    "        # do an actual gradient step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        for p in dmm.rnn.parameters():\n",
    "            p.data.clamp_(-rnn_clamp, rnn_clamp)\n",
    "            # p.data.clamp_(-0.001, 0.001)\n",
    "        # NaN_detect(dmm, epoch, message=\"step_After\")        \n",
    "\n",
    "        epoch_nll += loss\n",
    "    \n",
    "    # report training diagnostics\n",
    "    times.append(time.time())\n",
    "    losses.append(epoch_nll)\n",
    "    epoch_time = times[-1] - times[-2]\n",
    "    # logging.info(\"[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)\" %\n",
    "    #                 (epoch, epoch_nll / N_train_time_slices, epoch_time))\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(\"epoch %d time : %d sec\" % (epoch+1, int(epoch_time)))\n",
    "        print(\"        loss : %f \" % epoch_nll)\n",
    "        # torch.save(dmm.state_dict, \"DMM_dic\")\n",
    "\n",
    "    if epoch % checkpoint_freq == 0 :\n",
    "        saveDic = {\n",
    "            \"DMM_dic\": dmm.state_dict\n",
    "        }\n",
    "        torch.save(saveDic,\"saveData\\\\\" + now + \"\\dic_epoch%d\"%(epoch+1))\n",
    "    elif epoch == num_epochs-1:\n",
    "        saveDic = {\n",
    "            \"DMM_dic\": dmm.state_dict,\n",
    "            \"WGAN_Network_dic\": WN.state_dict,\n",
    "            \"epoch_times\": times,\n",
    "            \"losses\":losses,\n",
    "            # \"args\": args\n",
    "        }\n",
    "        torch.save(saveDic,\"saveData\\\\\" + now + \"\\dic_epoch%d\"%(epoch+1))\n",
    "\n",
    "        ## save plot ##\n",
    "        FS = 10\n",
    "        fig = plt.figure()\n",
    "        plt.rcParams[\"font.size\"] = FS\n",
    "        plt.plot(saveDic[\"losses\"])\n",
    "        plt.title(\"Wasserstein Loss\")\n",
    "        plt.xlabel(\"epoch\", fontsize=FS)\n",
    "        plt.ylabel(\"loss\", fontsize=FS)\n",
    "        fig.savefig(\"saveData\\\\\" + now + \"\\LOSS.png\")\n",
    "    \n"
   ]
  },
  {
   "source": [
    "# NaN detector"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaN_detect(NetWork, epoch, message=None):\n",
    "    flag = False\n",
    "    for name, site in NetWork.state_dict().items():\n",
    "        if any(torch.isnan(site.reshape(-1))):\n",
    "            flag = True\n",
    "            # if message != None:\n",
    "            #     print(message)\n",
    "            # print(\"Nan was detected!! at epoch %d\" % epoch)\n",
    "            # print(\"Para name is %s\" % name)\n",
    "    if flag:\n",
    "        if message != None:\n",
    "            print(message)\n",
    "        assert False"
   ]
  },
  {
   "source": [
    "# Generate audio data from trained DMM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Make and Save midi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_midi(song, length, path=\"\", name=\"default.mid\", BPM = 120, interval = 120, velocity = 127):\n",
    "    mid = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "    track.append(MetaMessage('set_tempo', tempo=mido.bpm2tempo(BPM)))\n",
    "    for tones in song:\n",
    "        which_tone = (tones == 1).nonzero().reshape(-1)\n",
    "        if len(which_tone) == 0:\n",
    "            track.append(Message('note_on', note=0, velocity=0, time=0))\n",
    "            track.append(Message('note_off', note=0, time=interval))\n",
    "        else:\n",
    "            for which in which_tone:\n",
    "                track.append(Message('note_on', note=int(which), velocity=velocity, time=0))\n",
    "            for which in which_tone:\n",
    "                track.append(Message('note_off', note=int(which), time=interval))\n",
    "    mid.save(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_midi(song, path=\"\", name=\"default.mid\", BPM = 120, velocity = 100):\n",
    "    pm = pretty_midi.PrettyMIDI(resolution=960, initial_tempo=BPM) #pretty_midiオブジェクトを作ります\n",
    "    instrument = pretty_midi.Instrument(0) #instrumentはトラックみたいなものです。\n",
    "    for i,tones in enumerate(song):\n",
    "        which_tone = (tones == 1).nonzero().reshape(-1)\n",
    "        print(which_tone)\n",
    "        if len(which_tone) == 0:\n",
    "            note = pretty_midi.Note(velocity=0, pitch=0, start=i, end=i+1) #noteはNoteOnEventとNoteOffEventに相当します。\n",
    "            instrument.notes.append(note)\n",
    "        else:\n",
    "            for which in which_tone:\n",
    "                note = pretty_midi.Note(velocity=velocity, pitch=int(which), start=i, end=i+1) #noteはNoteOnEventとNoteOffEventに相当します。\n",
    "                instrument.notes.append(note)\n",
    "                print(\"DONE\")\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    pm.write(os.path.join(path, name)) #midiファイルを書き込みます。\n",
    "\n",
    "\n",
    "def midi_to_wav(midi_name, midi_path=\"\", wav_name = None, wav_path=\"\", rate=44100):\n",
    "    if wav_name == None:\n",
    "        wav_name = os.path.splitext(midi_name)[0]+ \".wav\"\n",
    "        print(wav_name)\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_name) #midiファイルを読み込みます\n",
    "    data = np.array(midi_data.synthesize()*1e5, dtype = \"int16\")\n",
    "    write(os.path.join(wav_path, wav_name), rate, data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate X\n",
    "def generate_Xs(dmm, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths):\n",
    "    songs_dic = {}\n",
    "    generated_mini_batch = dmm(mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths)\n",
    "    for i, song in enumerate(generated_mini_batch):\n",
    "        tones_container = []\n",
    "        for time in range(mini_batch_seq_lengths[i]):\n",
    "            print(song[time])\n",
    "            p = dist.Bernoulli(probs=song[time])\n",
    "            tone = p.sample()\n",
    "            tones_container.append(tone)\n",
    "        tones_container = torch.stack(tones_container)\n",
    "        songs_dic.update([(\"song%02d\"%(i), tones_container)])\n",
    "    return songs_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'songs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f2e4985ef5bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mNo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msongs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"song%02d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mNo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlengthG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msave_as_midi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"Gen.mid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmidi_to_wav\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmidi_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Gen.mid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'songs' is not defined"
     ]
    }
   ],
   "source": [
    "No = 0\n",
    "song = songs[\"song%02d\"%No]\n",
    "lengthG = len(song)\n",
    "save_as_midi(song=song, name= \"Gen.mid\")\n",
    "midi_to_wav(midi_name=\"Gen.mid\")"
   ]
  },
  {
   "source": [
    "## Listen midi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_midi(name, path=\"\"):\n",
    "    ports = mido.get_output_names()\n",
    "    print(\"START\")\n",
    "    with mido.open_output(ports[0]) as outport:\n",
    "        for msg in mido.MidiFile(os.path.join(path,name)):\n",
    "            time.sleep(msg.time)\n",
    "            if not msg.is_meta:\n",
    "                # print(outport, msg)\n",
    "                outport.send(msg)\n",
    "    print(\"END\")"
   ]
  },
  {
   "source": [
    "## Prepare mini batch for genetating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = torch.randperm(N_train_data)\n",
    "# print(\"Proceeding: %.2f \" % (epoch*100/5000) + \"%\")\n",
    "\n",
    "# process each mini-batch; this is where we take gradient steps\n",
    "which_mini_batch = 0\n",
    "\n",
    "# compute which sequences in the training set we should grab\n",
    "mini_batch_start = (which_mini_batch * mini_batch_size)\n",
    "mini_batch_end = np.min([(which_mini_batch + 1) * mini_batch_size, N_train_data])\n",
    "mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n",
    "\n",
    "# grab a fully prepped mini-batch using the helper function in the data loader\n",
    "mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n",
    "    = poly.get_mini_batch(mini_batch_indices, training_data_sequences,\n",
    "                            training_seq_lengths, cuda=cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"saveData\", \"20210108_22_40\")\n",
    "# path = os.path.join(\"saveData\", \"20210115_12_21\")\n",
    "DMM_dics = torch.load(os.path.join(path,\"dic_epoch10000\"))\n",
    "DMM_dic = DMM_dics[\"DMM_dic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary from trained dmm\n",
    "# DMM_dic = torch.load(\"DMM_dic\")\n",
    "dmm = DMM()\n",
    "dmm.load_state_dict(DMM_dic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.0094e-20, 3.4407e-20, 5.9592e-22, 5.5047e-20, 1.1814e-21, 1.6529e-20,\n        2.7930e-21, 8.2927e-20, 8.1620e-21, 1.4497e-20, 4.3196e-20, 6.4315e-21,\n        7.3374e-21, 1.9560e-18, 4.2507e-20, 6.0453e-21, 5.3941e-21, 3.7229e-21,\n        3.2609e-22, 9.6292e-21, 9.2933e-20, 2.2613e-21, 4.4191e-21, 8.1315e-21,\n        2.3166e-21, 3.2474e-21, 1.4908e-20, 1.4407e-20, 8.6003e-21, 1.4590e-20,\n        6.0835e-22, 2.0210e-20, 1.1373e-20, 2.8997e-21, 1.6675e-19, 2.7123e-20,\n        6.2462e-21, 6.9517e-21, 1.8018e-21, 4.5659e-21, 1.1716e-20, 3.4917e-20,\n        1.8491e-20, 1.4841e-20, 5.4882e-20, 1.3823e-22, 1.3598e-21, 3.4827e-22,\n        2.9814e-20, 1.4945e-20, 2.3046e-20, 2.4473e-20, 9.9684e-21, 5.9925e-21,\n        5.2138e-21, 6.2475e-21, 3.6686e-21, 4.5369e-21, 3.2609e-20, 1.6676e-20,\n        1.6149e-20, 1.0000e+00, 2.2849e-21, 1.7974e-21, 3.9153e-21, 1.2455e-19,\n        5.1309e-03, 2.6532e-19, 2.4282e-21, 8.1504e-22, 2.6426e-20, 8.0360e-22,\n        2.9089e-22, 3.1272e-21, 1.5376e-20, 3.2662e-20, 3.6166e-20, 4.0183e-21,\n        1.6454e-20, 7.1495e-22, 2.3209e-20, 1.0941e-21, 8.4914e-21, 1.0243e-19,\n        6.5842e-21, 7.3536e-21, 6.8902e-21, 6.0270e-21],\n       grad_fn=<SelectBackward>)\ntensor([5.5253e-21, 3.0663e-20, 7.6671e-20, 1.0921e-21, 4.9248e-21, 1.3682e-21,\n        3.9683e-21, 1.3704e-19, 2.1897e-20, 4.0913e-20, 7.9860e-21, 7.3147e-21,\n        4.4295e-21, 1.9680e-23, 3.3623e-20, 1.2865e-20, 8.3014e-21, 1.5172e-21,\n        9.3217e-21, 3.6731e-19, 3.4054e-20, 1.0727e-21, 8.8380e-20, 1.8921e-21,\n        3.1639e-21, 6.1119e-21, 1.2882e-20, 9.1638e-22, 4.3404e-21, 1.7042e-21,\n        2.0451e-21, 2.7103e-20, 3.6182e-20, 1.7561e-19, 7.3181e-21, 3.1178e-20,\n        1.4556e-21, 3.8535e-21, 1.4336e-20, 2.4572e-22, 6.8468e-21, 2.9212e-19,\n        4.8896e-21, 5.5778e-20, 2.7071e-21, 1.8799e-20, 1.4990e-20, 1.0072e-20,\n        3.4802e-21, 1.9257e-20, 2.6255e-20, 6.8963e-20, 1.0863e-20, 1.2325e-20,\n        2.3521e-21, 4.1453e-22, 8.1542e-21, 6.6453e-20, 4.1238e-20, 6.8358e-21,\n        3.3216e-20, 1.0000e+00, 1.9403e-21, 2.7554e-20, 6.7852e-21, 4.6252e-20,\n        6.0739e-04, 1.8864e-20, 8.9502e-21, 5.9591e-21, 2.4575e-20, 1.0294e-19,\n        6.2650e-23, 4.9639e-20, 1.5791e-20, 6.3160e-21, 3.9822e-20, 2.3390e-20,\n        1.8353e-20, 7.9060e-21, 3.4889e-21, 2.3194e-22, 1.9814e-20, 1.4846e-21,\n        3.3745e-22, 1.7911e-20, 1.4290e-20, 1.4681e-20],\n       grad_fn=<SelectBackward>)\ntensor([7.3610e-21, 4.8778e-20, 2.6091e-19, 4.4459e-19, 1.5075e-20, 3.8549e-20,\n        6.5124e-20, 3.6057e-20, 1.4008e-19, 6.0714e-21, 1.7915e-20, 8.1073e-21,\n        1.0515e-21, 3.5890e-21, 1.4794e-20, 1.8090e-18, 5.9599e-22, 2.3834e-20,\n        1.3292e-20, 1.7489e-20, 2.5015e-20, 3.4380e-20, 5.7827e-21, 4.9673e-20,\n        7.5981e-21, 3.3238e-20, 1.6599e-21, 2.7924e-20, 4.8661e-21, 9.6311e-20,\n        9.3079e-21, 5.0519e-19, 1.1730e-20, 1.2506e-20, 3.6777e-21, 1.6110e-20,\n        9.9131e-21, 4.0778e-21, 5.3734e-20, 4.9196e-21, 1.8777e-19, 7.5887e-19,\n        1.1312e-19, 5.6771e-21, 2.2635e-21, 1.9987e-19, 3.7771e-20, 6.3556e-22,\n        1.5472e-21, 2.2971e-19, 1.8471e-20, 5.3997e-20, 1.0940e-20, 5.1455e-22,\n        1.1500e-20, 1.7608e-20, 2.5183e-20, 1.0641e-20, 1.0052e-18, 5.0042e-21,\n        9.3548e-21, 1.0000e+00, 8.7494e-21, 3.7426e-22, 5.9573e-22, 1.0137e-20,\n        1.0614e-01, 1.3461e-21, 9.0918e-21, 2.9481e-20, 8.5261e-22, 9.4746e-22,\n        3.5963e-21, 6.5757e-21, 1.2248e-19, 7.1242e-24, 6.9991e-21, 7.0291e-20,\n        1.5461e-19, 1.1169e-20, 4.2368e-21, 6.5933e-21, 1.9511e-22, 7.4062e-21,\n        1.3812e-21, 1.8190e-20, 5.3154e-21, 6.2308e-21],\n       grad_fn=<SelectBackward>)\ntensor([3.4236e-20, 7.6534e-21, 6.2048e-21, 1.0480e-20, 3.0573e-21, 2.5665e-20,\n        4.4220e-21, 3.7961e-20, 6.0469e-20, 6.5427e-21, 2.0047e-21, 6.9149e-22,\n        1.6799e-20, 3.7307e-22, 2.3739e-19, 1.6675e-18, 2.4974e-21, 1.6623e-22,\n        1.1256e-20, 8.0319e-20, 3.6694e-20, 1.4882e-21, 6.5329e-21, 2.4428e-22,\n        2.3405e-21, 1.6114e-20, 8.4493e-21, 2.0529e-20, 1.4622e-20, 8.1366e-22,\n        3.8283e-20, 1.2546e-21, 1.4417e-21, 1.8931e-22, 6.4224e-20, 2.0441e-18,\n        8.2473e-22, 2.5777e-20, 2.6855e-20, 1.9516e-13, 5.6684e-20, 2.0290e-20,\n        8.9791e-20, 6.7853e-09, 2.1108e-14, 4.4074e-19, 6.2021e-20, 2.8092e-21,\n        7.5788e-22, 8.2873e-13, 3.6290e-08, 5.8973e-21, 1.0991e-20, 6.2613e-13,\n        2.1164e-17, 2.2727e-20, 1.4102e-13, 1.1522e-20, 1.1227e-11, 1.7808e-12,\n        2.0431e-21, 9.9996e-01, 2.0758e-21, 1.1348e-19, 3.0304e-20, 1.5861e-10,\n        4.6344e-01, 7.1666e-15, 1.4698e-14, 8.1903e-21, 2.3186e-18, 5.4002e-21,\n        7.3016e-22, 7.4397e-21, 3.3486e-21, 1.3529e-20, 1.4719e-20, 1.4995e-21,\n        8.0616e-20, 2.5375e-21, 6.0467e-21, 9.6706e-22, 4.3077e-21, 1.8588e-20,\n        9.3108e-20, 1.8801e-21, 2.2869e-19, 7.4539e-21],\n       grad_fn=<SelectBackward>)\ntensor([1.3603e-11, 5.8728e-18, 3.7532e-15, 1.6592e-14, 6.7195e-14, 4.8780e-12,\n        1.0120e-13, 6.7187e-13, 8.6867e-15, 2.1921e-14, 3.7702e-15, 3.7104e-13,\n        4.6055e-12, 1.7555e-14, 4.6952e-14, 5.5935e-15, 5.4652e-17, 8.3895e-12,\n        6.8783e-13, 9.4372e-12, 2.1401e-11, 9.0390e-18, 4.7881e-11, 7.1821e-15,\n        3.9159e-13, 4.9481e-15, 2.5998e-14, 8.2618e-12, 9.5181e-17, 1.4504e-13,\n        9.2822e-13, 6.3259e-12, 1.0677e-12, 1.9624e-13, 5.4545e-08, 1.6129e-09,\n        2.0086e-12, 4.5499e-13, 1.0805e-11, 1.2962e-05, 9.2484e-19, 1.2292e-09,\n        2.5226e-10, 1.0415e-02, 3.6023e-05, 9.6303e-07, 1.1202e-15, 8.6137e-16,\n        3.2796e-20, 1.1146e-06, 3.1426e-05, 5.1379e-16, 1.3138e-08, 4.4023e-04,\n        5.2846e-07, 1.8944e-13, 2.2993e-05, 2.1227e-12, 4.4730e-06, 1.0914e-05,\n        4.3328e-13, 9.8446e-01, 8.9456e-20, 1.4743e-09, 9.4261e-12, 7.4999e-03,\n        8.6076e-01, 1.3953e-05, 4.3024e-06, 5.6826e-19, 1.3779e-08, 7.6983e-16,\n        1.3918e-11, 8.3250e-15, 2.1996e-15, 3.6926e-11, 7.2798e-14, 3.6122e-13,\n        2.7946e-12, 3.3698e-14, 7.1881e-18, 8.0834e-14, 1.2369e-12, 6.6428e-11,\n        2.0296e-17, 7.2470e-12, 1.9628e-11, 4.2645e-16],\n       grad_fn=<SelectBackward>)\ntensor([2.3713e-06, 8.5132e-11, 1.8701e-07, 3.7899e-06, 4.9208e-09, 2.6007e-06,\n        1.4311e-07, 4.5272e-07, 1.1128e-08, 8.7465e-08, 1.0967e-05, 8.2027e-07,\n        5.9847e-06, 7.8822e-07, 5.3923e-07, 4.5383e-07, 9.8783e-09, 3.4208e-06,\n        1.5087e-07, 6.1900e-07, 4.8271e-06, 1.2579e-10, 2.2707e-07, 2.4107e-09,\n        5.4610e-08, 2.0081e-08, 6.7134e-08, 3.5721e-07, 2.0160e-09, 1.3573e-06,\n        1.2944e-07, 1.8806e-06, 1.6755e-05, 1.1894e-07, 1.1580e-04, 3.2741e-04,\n        3.9749e-07, 2.9793e-07, 1.6091e-05, 1.2916e-01, 2.9672e-10, 2.9645e-06,\n        1.3071e-06, 2.7092e-02, 4.6096e-03, 6.5802e-05, 3.0368e-07, 1.8427e-08,\n        3.1541e-10, 2.6759e-03, 2.3929e-01, 9.7430e-07, 4.5645e-05, 1.7949e-02,\n        4.1391e-03, 6.0911e-07, 3.0368e-03, 4.8295e-06, 1.9068e-02, 1.1125e-02,\n        1.7457e-07, 9.3702e-01, 1.0439e-10, 3.5247e-03, 4.4535e-06, 1.6780e-01,\n        8.2099e-01, 4.2009e-04, 2.8035e-05, 1.5099e-08, 1.6175e-02, 1.1820e-07,\n        4.3617e-05, 1.5448e-06, 3.6495e-08, 6.9132e-07, 1.2009e-08, 4.3528e-06,\n        1.4705e-06, 5.0075e-08, 2.2799e-09, 4.7169e-07, 8.4551e-06, 1.3772e-05,\n        9.8014e-10, 1.2666e-07, 6.2411e-06, 1.9203e-08],\n       grad_fn=<SelectBackward>)\ntensor([3.6347e-04, 6.7379e-03, 1.0197e-06, 6.4283e-05, 1.8985e-04, 1.1257e-03,\n        1.7983e-05, 1.5683e-04, 4.2934e-04, 1.7149e-05, 2.4573e-06, 3.1193e-05,\n        1.3041e-04, 3.2825e-04, 1.5167e-06, 3.8847e-04, 2.2502e-05, 2.8575e-05,\n        8.5521e-06, 1.2322e-05, 3.5587e-04, 3.2558e-06, 4.5433e-04, 3.9104e-06,\n        5.0337e-05, 7.2617e-05, 2.2360e-06, 3.9559e-05, 3.2803e-06, 5.2051e-05,\n        7.9006e-05, 1.6222e-04, 1.9974e-04, 7.3817e-06, 1.3321e-03, 1.8752e-03,\n        2.0418e-03, 2.9428e-04, 4.1466e-04, 1.8252e-02, 1.3906e-07, 1.3644e-02,\n        5.2749e-04, 3.5111e-01, 2.1243e-01, 6.1246e-04, 2.5005e-04, 2.2121e-06,\n        2.6155e-06, 1.2474e-01, 5.0966e-01, 5.4385e-06, 1.6563e-04, 1.5115e-01,\n        2.4220e-01, 4.6847e-05, 2.1792e-02, 1.0108e-04, 6.1422e-02, 1.8940e-02,\n        5.2373e-05, 7.1822e-01, 4.2791e-05, 3.5005e-02, 2.6216e-04, 4.2514e-01,\n        3.9428e-01, 1.6054e-02, 5.5847e-03, 1.1948e-07, 1.7927e-03, 5.7869e-05,\n        5.6414e-04, 5.6851e-05, 7.6930e-05, 9.4797e-04, 4.3401e-06, 1.0259e-04,\n        7.2410e-07, 2.8106e-06, 5.3241e-06, 1.0396e-05, 3.0914e-05, 2.1693e-03,\n        3.9699e-06, 7.4713e-05, 7.1995e-05, 1.7143e-04],\n       grad_fn=<SelectBackward>)\ntensor([6.2464e-05, 1.2666e-07, 1.2788e-05, 1.0662e-04, 1.7529e-05, 4.2798e-04,\n        6.6472e-07, 3.3615e-04, 1.1450e-05, 1.7044e-03, 1.3502e-05, 6.3198e-03,\n        1.7949e-04, 6.1424e-04, 2.2115e-03, 3.1972e-05, 6.7231e-05, 6.2201e-04,\n        3.5460e-04, 4.8436e-04, 6.8960e-03, 8.2636e-06, 7.5999e-04, 1.1133e-04,\n        3.1128e-03, 5.6275e-05, 2.3114e-05, 3.8220e-05, 2.0713e-06, 5.2023e-06,\n        4.1218e-04, 8.3978e-04, 1.3444e-04, 1.1889e-03, 6.6573e-02, 3.0756e-02,\n        1.4885e-03, 2.8350e-04, 1.2705e-02, 9.0441e-01, 6.3970e-05, 2.6250e-03,\n        6.0247e-03, 1.9590e-01, 1.8665e-02, 3.8666e-03, 3.9965e-04, 5.5044e-06,\n        3.5296e-06, 3.8772e-02, 7.4690e-01, 9.7253e-05, 1.1956e-03, 3.7573e-01,\n        1.2746e-02, 5.9638e-06, 3.6030e-01, 1.2571e-04, 1.0696e-01, 1.6028e-02,\n        2.8234e-03, 5.1240e-01, 9.3448e-06, 1.3534e-02, 6.3825e-03, 2.2058e-02,\n        1.7887e-01, 1.6771e-01, 5.5389e-02, 1.8322e-05, 3.7039e-04, 3.8224e-05,\n        4.1311e-04, 1.0593e-03, 1.8130e-04, 1.5450e-03, 7.0576e-04, 6.7792e-04,\n        6.3877e-04, 4.5894e-04, 1.1597e-05, 5.7460e-05, 8.3925e-05, 8.6216e-04,\n        4.2789e-06, 1.1645e-04, 3.5109e-04, 2.9998e-05],\n       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "songs = generate_Xs(dmm, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(songs[\"song%02d\"%No])\n",
    "print(mini_batch[No])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "save_as_midi() got an unexpected keyword argument 'length'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7906dc822b38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msongs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"song%02d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mNo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlengthG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msave_as_midi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlengthG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"Gen.mid\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m360\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: save_as_midi() got an unexpected keyword argument 'length'"
     ]
    }
   ],
   "source": [
    "No = 0\n",
    "song = songs[\"song%02d\"%No]\n",
    "lengthG = len(song)\n",
    "save_as_midi(song=song, length = lengthG, name= \"Gen.mid\",interval=360)\n",
    "\n",
    "song = mini_batch[No]\n",
    "lengthT = mini_batch_seq_lengths[No]\n",
    "save_as_midi(song=song, length = lengthT, name=\"Tra.mid\", interval=360)\n",
    "\n",
    "# song = training_data_sequences[0]\n",
    "# lengthT = 8\n",
    "# save_as_midi(song=song, length = lengthT, name=\"Tra.mid\",interval=240)\n",
    "\n",
    "if lengthG == lengthT:\n",
    "    print(lengthG)\n",
    "else:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START\n",
      "END\n",
      "START\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "# path = os.path.join(\"saveData\", \"20210108_22_40\")\n",
    "path = os.path.join(\"saveData\", \"20210115_12_21\")\n",
    "epoch_number = 20000\n",
    "# listen_midi(\"test.mid\")\n",
    "# listen_midi(\"Gen.mid\")\n",
    "listen_midi(os.path.join(path, \"Training_Epoch%d.mid\"%epoch_number))\n",
    "listen_midi(os.path.join(path, \"Generated_Epoch%d.mid\"%epoch_number))"
   ]
  },
  {
   "source": [
    "# Dipict Loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-01-08T21:04:04.857277</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 392.14375 277.314375 \r\nL 392.14375 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 50.14375 239.758125 \r\nL 384.94375 239.758125 \r\nL 384.94375 22.318125 \r\nL 50.14375 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6ed39c6418\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m6ed39c6418\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(62.180682 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.234659\" xlink:href=\"#m6ed39c6418\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(116.690909 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.107386\" xlink:href=\"#m6ed39c6418\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(177.563636 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"247.980114\" xlink:href=\"#m6ed39c6418\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(238.436364 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"308.852841\" xlink:href=\"#m6ed39c6418\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(299.309091 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m6ed39c6418\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(357.000568 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- epoch -->\r\n     <g transform=\"translate(202.315625 268.034687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma7dc55f4fb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"228.467798\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.00 -->\r\n      <g transform=\"translate(20.878125 232.267016)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"199.224255\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.05 -->\r\n      <g transform=\"translate(20.878125 203.023474)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"169.980713\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.10 -->\r\n      <g transform=\"translate(20.878125 173.779932)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"140.737171\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.15 -->\r\n      <g transform=\"translate(20.878125 144.53639)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"111.493629\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.20 -->\r\n      <g transform=\"translate(20.878125 115.292848)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"82.250087\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(20.878125 86.049306)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"53.006545\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.30 -->\r\n      <g transform=\"translate(20.878125 56.805764)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#ma7dc55f4fb\" y=\"23.763003\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.35 -->\r\n      <g transform=\"translate(20.878125 27.562222)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(14.798438 140.695937)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p7cb26e8063)\" d=\"M 65.361932 223.17184 \r\nL 65.666295 222.612018 \r\nL 66.275023 218.80949 \r\nL 66.579386 216.477261 \r\nL 66.88375 215.467989 \r\nL 67.188114 212.482066 \r\nL 67.492477 212.043449 \r\nL 67.796841 210.404018 \r\nL 68.101205 206.917507 \r\nL 68.405568 206.040506 \r\nL 68.709932 204.610261 \r\nL 69.014295 204.173921 \r\nL 69.318659 201.025857 \r\nL 69.623023 199.232548 \r\nL 69.927386 198.212155 \r\nL 70.23175 194.873437 \r\nL 70.536114 192.766489 \r\nL 70.840477 191.465903 \r\nL 71.753568 184.533305 \r\nL 72.057932 185.559679 \r\nL 72.362295 180.556269 \r\nL 72.666659 180.236201 \r\nL 72.971023 176.515533 \r\nL 73.275386 174.911991 \r\nL 73.57975 172.355118 \r\nL 73.884114 166.837783 \r\nL 74.188477 168.369868 \r\nL 74.492841 166.678176 \r\nL 74.797205 161.846404 \r\nL 75.101568 159.46764 \r\nL 75.405932 155.504008 \r\nL 75.710295 158.119334 \r\nL 76.014659 151.263113 \r\nL 76.319023 147.345562 \r\nL 76.623386 148.847567 \r\nL 77.232114 145.728542 \r\nL 77.840841 137.787141 \r\nL 78.145205 138.396277 \r\nL 78.449568 132.89275 \r\nL 78.753932 136.780399 \r\nL 79.058295 131.775367 \r\nL 79.362659 124.738566 \r\nL 79.667023 126.556769 \r\nL 79.971386 125.979619 \r\nL 80.27575 117.042744 \r\nL 80.580114 118.126329 \r\nL 80.884477 112.470878 \r\nL 81.188841 113.963034 \r\nL 81.493205 113.095675 \r\nL 81.797568 112.986098 \r\nL 82.101932 104.265448 \r\nL 82.406295 105.287382 \r\nL 82.710659 105.3747 \r\nL 83.015023 102.005931 \r\nL 83.319386 102.119134 \r\nL 83.928114 94.499979 \r\nL 84.232477 91.381051 \r\nL 84.536841 89.500072 \r\nL 84.841205 84.787874 \r\nL 85.145568 91.807959 \r\nL 85.754295 79.618917 \r\nL 86.058659 82.747127 \r\nL 86.363023 80.369719 \r\nL 86.667386 78.88382 \r\nL 86.97175 80.395464 \r\nL 87.276114 73.343149 \r\nL 87.580477 70.558538 \r\nL 87.884841 71.502087 \r\nL 88.189205 64.258629 \r\nL 88.493568 66.473263 \r\nL 88.797932 71.594521 \r\nL 89.102295 68.572105 \r\nL 89.406659 57.310846 \r\nL 89.711023 54.009682 \r\nL 90.015386 48.583076 \r\nL 90.31975 57.610337 \r\nL 90.624114 52.936555 \r\nL 90.928477 53.523336 \r\nL 91.232841 44.615108 \r\nL 91.537205 49.662966 \r\nL 91.841568 50.686607 \r\nL 92.450295 42.52505 \r\nL 92.754659 43.474229 \r\nL 93.059023 37.759462 \r\nL 93.363386 37.08145 \r\nL 93.66775 38.660671 \r\nL 93.972114 37.603267 \r\nL 94.276477 34.589654 \r\nL 94.580841 40.340781 \r\nL 94.885205 39.974845 \r\nL 95.189568 37.586185 \r\nL 95.798295 32.201761 \r\nL 96.102659 46.664692 \r\nL 96.711386 33.489806 \r\nL 97.01575 37.582769 \r\nL 97.320114 49.98862 \r\nL 97.624477 45.123887 \r\nL 97.928841 45.50314 \r\nL 98.233205 55.480224 \r\nL 99.146295 71.213995 \r\nL 99.450659 69.431534 \r\nL 99.755023 76.160512 \r\nL 100.059386 72.30586 \r\nL 100.668114 91.581362 \r\nL 100.972477 91.249215 \r\nL 101.276841 88.482209 \r\nL 101.581205 98.65251 \r\nL 101.885568 95.68738 \r\nL 102.189932 101.037675 \r\nL 102.798659 114.604747 \r\nL 103.103023 114.025174 \r\nL 103.407386 118.966261 \r\nL 103.71175 127.707409 \r\nL 104.016114 119.33137 \r\nL 104.320477 125.556223 \r\nL 104.624841 124.594773 \r\nL 104.929205 136.663867 \r\nL 105.233568 132.455523 \r\nL 105.537932 138.623353 \r\nL 105.842295 141.652392 \r\nL 106.146659 151.670438 \r\nL 106.451023 148.816532 \r\nL 106.755386 155.27498 \r\nL 107.05975 153.350495 \r\nL 107.364114 158.437942 \r\nL 107.668477 153.984155 \r\nL 107.972841 163.85062 \r\nL 108.277205 160.889225 \r\nL 108.581568 165.917168 \r\nL 108.885932 163.390346 \r\nL 109.190295 169.63375 \r\nL 109.494659 165.241771 \r\nL 109.799023 171.456066 \r\nL 110.103386 168.986991 \r\nL 110.712114 177.03694 \r\nL 111.320841 180.639565 \r\nL 111.625205 182.316715 \r\nL 111.929568 186.988231 \r\nL 112.233932 189.63593 \r\nL 112.538295 190.560911 \r\nL 112.842659 188.346861 \r\nL 113.147023 195.45145 \r\nL 113.451386 189.560321 \r\nL 114.060114 195.10499 \r\nL 114.364477 191.542815 \r\nL 114.668841 191.813001 \r\nL 114.973205 194.876051 \r\nL 115.277568 193.759915 \r\nL 115.581932 200.286272 \r\nL 115.886295 201.002169 \r\nL 116.190659 201.33809 \r\nL 116.495023 200.478365 \r\nL 116.799386 201.121028 \r\nL 117.10375 202.48395 \r\nL 117.408114 205.963328 \r\nL 117.712477 204.62931 \r\nL 118.016841 204.361203 \r\nL 118.321205 203.5356 \r\nL 118.625568 204.968726 \r\nL 118.929932 203.344869 \r\nL 119.234295 209.078295 \r\nL 119.538659 208.877376 \r\nL 119.843023 211.478824 \r\nL 120.45175 206.499055 \r\nL 121.060477 207.95233 \r\nL 121.364841 212.590974 \r\nL 121.669205 214.118521 \r\nL 121.973568 210.377224 \r\nL 122.277932 209.165613 \r\nL 122.582295 214.942398 \r\nL 122.886659 214.243823 \r\nL 123.191023 217.545855 \r\nL 123.495386 212.157506 \r\nL 123.79975 220.693108 \r\nL 124.104114 223.038128 \r\nL 124.408477 220.059049 \r\nL 124.712841 220.626652 \r\nL 125.017205 224.263546 \r\nL 125.625932 224.347784 \r\nL 125.930295 221.472731 \r\nL 126.234659 220.492387 \r\nL 126.539023 221.169614 \r\nL 126.843386 223.196364 \r\nL 127.14775 222.865356 \r\nL 127.452114 224.005157 \r\nL 127.756477 223.137419 \r\nL 128.060841 224.679475 \r\nL 128.365205 223.225625 \r\nL 128.669568 224.464944 \r\nL 128.973932 224.642772 \r\nL 129.278295 223.433167 \r\nL 129.582659 225.331762 \r\nL 129.887023 227.943047 \r\nL 130.191386 225.105231 \r\nL 130.49575 225.792726 \r\nL 130.800114 224.125035 \r\nL 131.408841 227.410233 \r\nL 131.713205 225.422911 \r\nL 132.017568 227.915281 \r\nL 132.321932 224.45857 \r\nL 132.626295 227.515312 \r\nL 132.930659 227.457469 \r\nL 133.235023 223.682284 \r\nL 133.539386 227.987975 \r\nL 133.84375 227.255908 \r\nL 134.148114 227.186661 \r\nL 134.452477 229.451827 \r\nL 134.756841 226.649839 \r\nL 135.061205 226.191631 \r\nL 135.365568 226.596731 \r\nL 135.669932 228.360224 \r\nL 135.974295 226.881593 \r\nL 136.278659 226.716244 \r\nL 136.583023 229.874489 \r\nL 136.887386 228.033203 \r\nL 137.19175 227.489823 \r\nL 137.496114 228.147453 \r\nL 137.800477 227.482606 \r\nL 138.104841 228.010987 \r\nL 138.409205 227.890277 \r\nL 138.713568 228.190236 \r\nL 139.017932 226.652929 \r\nL 139.322295 226.921977 \r\nL 139.626659 226.545524 \r\nL 139.931023 229.249322 \r\nL 140.235386 228.251277 \r\nL 140.53975 227.660128 \r\nL 140.844114 228.582685 \r\nL 141.148477 227.147081 \r\nL 141.452841 228.308549 \r\nL 141.757205 227.521683 \r\nL 142.061568 228.632453 \r\nL 142.365932 226.962061 \r\nL 142.670295 228.193451 \r\nL 142.974659 227.910585 \r\nL 143.279023 227.839631 \r\nL 143.583386 228.351362 \r\nL 143.88775 227.560095 \r\nL 144.192114 228.503462 \r\nL 144.496477 227.958266 \r\nL 144.800841 228.073097 \r\nL 145.409568 227.701328 \r\nL 146.018295 227.725911 \r\nL 146.322659 228.24285 \r\nL 146.627023 226.756613 \r\nL 146.931386 227.169612 \r\nL 147.23575 229.043967 \r\nL 147.540114 227.361094 \r\nL 147.844477 227.653219 \r\nL 148.148841 227.604408 \r\nL 148.453205 226.743453 \r\nL 148.757568 227.377054 \r\nL 149.061932 228.624542 \r\nL 149.366295 227.732093 \r\nL 149.670659 227.468498 \r\nL 149.975023 227.965531 \r\nL 150.279386 227.506455 \r\nL 150.888114 227.877484 \r\nL 151.192477 226.856231 \r\nL 151.496841 227.946493 \r\nL 151.801205 227.297775 \r\nL 152.105568 227.804089 \r\nL 152.409932 227.053958 \r\nL 153.018659 227.939318 \r\nL 153.323023 226.608745 \r\nL 153.627386 226.22464 \r\nL 153.93175 226.364914 \r\nL 154.236114 227.21872 \r\nL 154.540477 226.466136 \r\nL 154.844841 226.996312 \r\nL 155.149205 226.573447 \r\nL 155.453568 226.570322 \r\nL 155.757932 225.614376 \r\nL 156.062295 226.408876 \r\nL 156.366659 226.309914 \r\nL 156.975386 226.650984 \r\nL 157.27975 226.602481 \r\nL 157.584114 226.057989 \r\nL 157.888477 225.751205 \r\nL 158.192841 226.026647 \r\nL 158.497205 226.049402 \r\nL 158.801568 225.957632 \r\nL 159.105932 225.15569 \r\nL 159.410295 225.467694 \r\nL 159.714659 225.202781 \r\nL 160.019023 226.308696 \r\nL 160.323386 225.686844 \r\nL 160.62775 225.638281 \r\nL 160.932114 225.109658 \r\nL 161.236477 225.219803 \r\nL 161.540841 224.827752 \r\nL 161.845205 225.906821 \r\nL 162.149568 224.872841 \r\nL 162.453932 224.898638 \r\nL 162.758295 225.636542 \r\nL 163.062659 224.310845 \r\nL 163.367023 225.273289 \r\nL 163.671386 225.185466 \r\nL 163.97575 224.669821 \r\nL 164.280114 224.554215 \r\nL 164.888841 224.895968 \r\nL 165.193205 222.589378 \r\nL 165.497568 225.000371 \r\nL 165.801932 225.170642 \r\nL 166.106295 224.581889 \r\nL 166.410659 223.623103 \r\nL 166.715023 223.793266 \r\nL 167.019386 223.273016 \r\nL 167.628114 223.607408 \r\nL 168.236841 223.377084 \r\nL 168.541205 223.000223 \r\nL 168.845568 223.124608 \r\nL 169.149932 223.543368 \r\nL 169.454295 224.433051 \r\nL 169.758659 223.640209 \r\nL 170.063023 223.580825 \r\nL 170.367386 223.759672 \r\nL 170.67175 222.660409 \r\nL 170.976114 222.359168 \r\nL 171.280477 223.405179 \r\nL 171.584841 222.169333 \r\nL 171.889205 223.19129 \r\nL 172.497932 223.734548 \r\nL 172.802295 222.104758 \r\nL 173.106659 222.154166 \r\nL 173.411023 222.701608 \r\nL 173.715386 222.681527 \r\nL 174.01975 222.863935 \r\nL 174.324114 222.902386 \r\nL 174.932841 221.22758 \r\nL 175.237205 222.309661 \r\nL 175.541568 222.41576 \r\nL 175.845932 221.634177 \r\nL 176.150295 222.770088 \r\nL 176.454659 222.13493 \r\nL 176.759023 222.147349 \r\nL 177.36775 221.869836 \r\nL 177.672114 222.088399 \r\nL 177.976477 221.825926 \r\nL 178.280841 220.990606 \r\nL 178.585205 221.735631 \r\nL 178.889568 221.659195 \r\nL 179.193932 221.959249 \r\nL 179.498295 221.6253 \r\nL 179.802659 220.429326 \r\nL 180.107023 221.778439 \r\nL 180.411386 222.080736 \r\nL 181.020114 220.92086 \r\nL 181.324477 220.827451 \r\nL 181.933205 221.199155 \r\nL 182.237568 220.77438 \r\nL 182.846295 221.372059 \r\nL 183.150659 220.86797 \r\nL 183.455023 221.409345 \r\nL 183.759386 220.640754 \r\nL 184.06375 221.232114 \r\nL 184.368114 218.994613 \r\nL 184.672477 220.763361 \r\nL 184.976841 220.616159 \r\nL 185.281205 220.633323 \r\nL 185.585568 220.508578 \r\nL 185.889932 220.876825 \r\nL 186.194295 221.01176 \r\nL 186.498659 220.114926 \r\nL 186.803023 219.736542 \r\nL 187.107386 220.220707 \r\nL 187.41175 220.412115 \r\nL 187.716114 220.182967 \r\nL 188.020477 220.348608 \r\nL 188.324841 221.340218 \r\nL 188.629205 220.021899 \r\nL 188.933568 219.960992 \r\nL 189.237932 219.332921 \r\nL 189.542295 219.878663 \r\nL 190.151023 218.994065 \r\nL 190.455386 220.285087 \r\nL 190.75975 220.381927 \r\nL 191.064114 219.517113 \r\nL 191.368477 219.912132 \r\nL 191.672841 220.932049 \r\nL 191.977205 219.90331 \r\nL 192.281568 219.276777 \r\nL 192.585932 219.783493 \r\nL 192.890295 219.695242 \r\nL 193.194659 220.08109 \r\nL 193.499023 220.914861 \r\nL 193.803386 221.023436 \r\nL 194.10775 221.723941 \r\nL 194.412114 220.566972 \r\nL 195.020841 225.56421 \r\nL 195.325205 229.724209 \r\nL 195.629568 229.836583 \r\nL 195.933932 227.695885 \r\nL 196.238295 227.60732 \r\nL 196.542659 229.661053 \r\nL 196.847023 229.235649 \r\nL 197.151386 228.572014 \r\nL 197.45575 227.499987 \r\nL 197.760114 227.519032 \r\nL 198.064477 227.988698 \r\nL 198.368841 226.001133 \r\nL 198.673205 226.921289 \r\nL 198.977568 227.342375 \r\nL 199.281932 225.487881 \r\nL 199.586295 224.854182 \r\nL 199.890659 224.990318 \r\nL 200.195023 223.86167 \r\nL 200.499386 224.909142 \r\nL 200.80375 224.625619 \r\nL 201.108114 224.106856 \r\nL 201.412477 225.72733 \r\nL 201.716841 222.725553 \r\nL 202.021205 222.608449 \r\nL 202.325568 222.791418 \r\nL 202.629932 223.75254 \r\nL 202.934295 222.235069 \r\nL 203.238659 223.439836 \r\nL 203.847386 223.470373 \r\nL 204.456114 222.784682 \r\nL 205.064841 227.797698 \r\nL 205.369205 226.370441 \r\nL 205.673568 226.797264 \r\nL 205.977932 224.848175 \r\nL 206.282295 223.757686 \r\nL 206.586659 224.283296 \r\nL 207.49975 222.234359 \r\nL 207.804114 220.967175 \r\nL 208.108477 221.172985 \r\nL 208.412841 220.168645 \r\nL 209.325932 218.753779 \r\nL 209.630295 217.710238 \r\nL 210.239023 216.737984 \r\nL 210.543386 216.693584 \r\nL 211.152114 215.638248 \r\nL 211.456477 215.289713 \r\nL 212.065205 213.705113 \r\nL 212.369568 214.229865 \r\nL 212.978295 212.302308 \r\nL 213.282659 212.158558 \r\nL 213.587023 210.963639 \r\nL 213.891386 210.478317 \r\nL 214.19575 210.330878 \r\nL 214.500114 210.313138 \r\nL 214.804477 209.414281 \r\nL 215.717568 207.830131 \r\nL 216.021932 207.521206 \r\nL 216.326295 206.899107 \r\nL 216.630659 207.08965 \r\nL 216.935023 206.539034 \r\nL 217.239386 205.638944 \r\nL 217.54375 206.191042 \r\nL 217.848114 205.9917 \r\nL 218.152477 204.818928 \r\nL 218.456841 205.697992 \r\nL 218.761205 204.30975 \r\nL 219.674295 202.942987 \r\nL 219.978659 202.972104 \r\nL 220.283023 202.285039 \r\nL 220.587386 202.046357 \r\nL 220.89175 202.168774 \r\nL 221.196114 201.183547 \r\nL 221.500477 202.408861 \r\nL 221.804841 202.738185 \r\nL 222.109205 202.437803 \r\nL 222.413568 200.198834 \r\nL 222.717932 202.108647 \r\nL 223.326659 199.834116 \r\nL 223.631023 201.673446 \r\nL 223.935386 199.836713 \r\nL 224.23975 200.17739 \r\nL 224.544114 200.203427 \r\nL 224.848477 199.579376 \r\nL 225.152841 201.346112 \r\nL 225.457205 201.508125 \r\nL 225.761568 199.159995 \r\nL 226.065932 200.396152 \r\nL 226.370295 199.459885 \r\nL 226.674659 199.321395 \r\nL 226.979023 202.058438 \r\nL 227.283386 199.203946 \r\nL 227.58775 199.420396 \r\nL 227.892114 199.909958 \r\nL 228.196477 199.839099 \r\nL 228.500841 201.355562 \r\nL 228.805205 200.57876 \r\nL 229.109568 201.00557 \r\nL 229.413932 200.24384 \r\nL 229.718295 204.579293 \r\nL 230.022659 204.425273 \r\nL 230.327023 203.560709 \r\nL 230.631386 203.441406 \r\nL 230.93575 204.45804 \r\nL 231.544477 201.457519 \r\nL 231.848841 201.243773 \r\nL 232.153205 200.067887 \r\nL 232.457568 200.016844 \r\nL 232.761932 200.101103 \r\nL 233.066295 199.440918 \r\nL 233.370659 200.160467 \r\nL 233.675023 198.742776 \r\nL 233.979386 198.31857 \r\nL 234.588114 197.856442 \r\nL 234.892477 198.366911 \r\nL 235.501205 197.634407 \r\nL 235.805568 198.480737 \r\nL 236.109932 200.212735 \r\nL 236.414295 196.584461 \r\nL 236.718659 196.654153 \r\nL 237.023023 197.577621 \r\nL 237.327386 197.901273 \r\nL 238.240477 196.898764 \r\nL 238.544841 197.501816 \r\nL 238.849205 196.835598 \r\nL 239.153568 196.874152 \r\nL 239.762295 195.956015 \r\nL 240.066659 194.563241 \r\nL 240.371023 197.470698 \r\nL 240.675386 195.658019 \r\nL 240.97975 195.927294 \r\nL 241.284114 195.834511 \r\nL 241.892841 196.916482 \r\nL 242.197205 196.379929 \r\nL 242.501568 201.840444 \r\nL 242.805932 202.042008 \r\nL 243.110295 201.453927 \r\nL 243.414659 200.001181 \r\nL 243.719023 200.827688 \r\nL 244.023386 199.687107 \r\nL 244.32775 200.544224 \r\nL 244.632114 199.43002 \r\nL 244.936477 199.995065 \r\nL 245.240841 201.282258 \r\nL 245.545205 200.491405 \r\nL 245.849568 200.860152 \r\nL 246.153932 200.641952 \r\nL 246.458295 200.141052 \r\nL 246.762659 200.39891 \r\nL 247.067023 199.057234 \r\nL 247.371386 200.237305 \r\nL 247.67575 200.448879 \r\nL 247.980114 202.119537 \r\nL 248.284477 200.631838 \r\nL 248.588841 200.348163 \r\nL 248.893205 198.864252 \r\nL 249.197568 199.059663 \r\nL 249.501932 202.06136 \r\nL 249.806295 198.664527 \r\nL 250.415023 200.972664 \r\nL 250.719386 199.33666 \r\nL 251.02375 199.842483 \r\nL 251.328114 199.850187 \r\nL 251.632477 199.484615 \r\nL 251.936841 200.201026 \r\nL 252.241205 198.334857 \r\nL 252.545568 198.551836 \r\nL 252.849932 197.090238 \r\nL 253.154295 197.915501 \r\nL 253.458659 197.422202 \r\nL 253.763023 199.233025 \r\nL 254.067386 198.098397 \r\nL 254.37175 198.629289 \r\nL 254.676114 197.803808 \r\nL 254.980477 198.375267 \r\nL 255.284841 197.123458 \r\nL 255.589205 199.336795 \r\nL 255.893568 198.83033 \r\nL 256.197932 199.291258 \r\nL 256.502295 198.888393 \r\nL 256.806659 199.743137 \r\nL 257.111023 198.633627 \r\nL 257.415386 198.236488 \r\nL 257.71975 199.639757 \r\nL 258.024114 199.6307 \r\nL 258.328477 198.913212 \r\nL 258.632841 199.71719 \r\nL 258.937205 197.765849 \r\nL 259.241568 199.277087 \r\nL 259.545932 197.966382 \r\nL 259.850295 199.599665 \r\nL 260.459023 199.236611 \r\nL 260.763386 198.578078 \r\nL 261.06775 198.416466 \r\nL 261.372114 199.911815 \r\nL 261.676477 198.113202 \r\nL 261.980841 197.77544 \r\nL 262.285205 198.807095 \r\nL 262.589568 199.173284 \r\nL 262.893932 197.369455 \r\nL 263.198295 198.233331 \r\nL 263.502659 200.37037 \r\nL 263.807023 199.163063 \r\nL 264.111386 197.194762 \r\nL 264.41575 199.60414 \r\nL 264.720114 199.06759 \r\nL 265.024477 198.141574 \r\nL 265.328841 199.243324 \r\nL 265.633205 201.390022 \r\nL 265.937568 199.665811 \r\nL 266.241932 200.620655 \r\nL 266.546295 199.763614 \r\nL 266.850659 198.095048 \r\nL 267.155023 198.859249 \r\nL 267.459386 198.960884 \r\nL 267.76375 199.676398 \r\nL 268.068114 201.038756 \r\nL 268.372477 199.865748 \r\nL 268.981205 200.376237 \r\nL 269.285568 204.381283 \r\nL 269.589932 205.330219 \r\nL 269.894295 206.778607 \r\nL 270.198659 206.989817 \r\nL 270.503023 206.508269 \r\nL 270.807386 206.314725 \r\nL 271.11175 206.302844 \r\nL 271.416114 208.279187 \r\nL 271.720477 204.395802 \r\nL 272.024841 207.452841 \r\nL 272.329205 206.353695 \r\nL 272.633568 204.248922 \r\nL 272.937932 207.873856 \r\nL 273.242295 204.900082 \r\nL 273.546659 209.867566 \r\nL 273.851023 206.64517 \r\nL 274.155386 205.771673 \r\nL 274.45975 209.562255 \r\nL 275.068477 206.465599 \r\nL 275.372841 211.304346 \r\nL 275.677205 210.952305 \r\nL 275.981568 211.138466 \r\nL 276.285932 212.342636 \r\nL 276.590295 211.417796 \r\nL 276.894659 209.360661 \r\nL 277.199023 208.853104 \r\nL 277.503386 209.726008 \r\nL 277.80775 208.077278 \r\nL 278.112114 208.379839 \r\nL 278.416477 207.944963 \r\nL 278.720841 208.499883 \r\nL 279.025205 207.182285 \r\nL 279.329568 207.585871 \r\nL 279.633932 207.249227 \r\nL 279.938295 206.297205 \r\nL 280.242659 204.744853 \r\nL 280.547023 207.305771 \r\nL 281.15575 204.484951 \r\nL 281.460114 204.999155 \r\nL 281.764477 203.749078 \r\nL 282.068841 205.415716 \r\nL 282.373205 202.779037 \r\nL 282.677568 205.063136 \r\nL 282.981932 202.973799 \r\nL 283.286295 203.349065 \r\nL 283.590659 202.827817 \r\nL 283.895023 203.231318 \r\nL 284.199386 200.947337 \r\nL 284.808114 202.54799 \r\nL 285.112477 202.108438 \r\nL 285.416841 203.103156 \r\nL 285.721205 201.787797 \r\nL 286.025568 202.684647 \r\nL 286.329932 200.84295 \r\nL 286.634295 203.041491 \r\nL 286.938659 201.741737 \r\nL 287.243023 201.236862 \r\nL 287.547386 201.097351 \r\nL 287.85175 204.448421 \r\nL 288.460477 200.176839 \r\nL 288.764841 203.211484 \r\nL 289.069205 201.071978 \r\nL 289.373568 200.86703 \r\nL 289.982295 201.77877 \r\nL 290.286659 199.225203 \r\nL 290.591023 201.612686 \r\nL 291.19975 201.983993 \r\nL 291.504114 200.890222 \r\nL 291.808477 202.291744 \r\nL 292.112841 199.010886 \r\nL 292.417205 200.371302 \r\nL 292.721568 202.259146 \r\nL 293.330295 200.202804 \r\nL 293.634659 201.208701 \r\nL 293.939023 201.535475 \r\nL 294.243386 198.118551 \r\nL 294.54775 201.777742 \r\nL 294.852114 199.44108 \r\nL 295.156477 201.382433 \r\nL 295.460841 200.055361 \r\nL 295.765205 203.772411 \r\nL 296.069568 200.059627 \r\nL 296.373932 201.282543 \r\nL 296.678295 203.299809 \r\nL 296.982659 200.797738 \r\nL 297.287023 203.138296 \r\nL 297.591386 204.439084 \r\nL 297.89575 206.528957 \r\nL 298.200114 205.875345 \r\nL 298.504477 208.820193 \r\nL 298.808841 209.654905 \r\nL 299.113205 216.952771 \r\nL 299.417568 212.008784 \r\nL 299.721932 213.213598 \r\nL 300.026295 212.927053 \r\nL 300.330659 213.801208 \r\nL 300.635023 212.429151 \r\nL 300.939386 211.879937 \r\nL 301.24375 212.958947 \r\nL 301.852477 210.010317 \r\nL 302.156841 213.562972 \r\nL 302.461205 211.869642 \r\nL 302.765568 211.002665 \r\nL 303.069932 210.745188 \r\nL 303.678659 212.04726 \r\nL 303.983023 211.725854 \r\nL 304.287386 213.331858 \r\nL 304.59175 213.97205 \r\nL 304.896114 224.671942 \r\nL 305.504841 224.670947 \r\nL 305.809205 223.035075 \r\nL 306.113568 224.694871 \r\nL 306.722295 219.082148 \r\nL 307.026659 220.938804 \r\nL 307.331023 220.49671 \r\nL 307.635386 221.413073 \r\nL 307.93975 219.094397 \r\nL 308.244114 217.956485 \r\nL 308.548477 219.073226 \r\nL 308.852841 220.636512 \r\nL 309.157205 218.107557 \r\nL 309.461568 218.107959 \r\nL 309.765932 219.662693 \r\nL 310.070295 216.647431 \r\nL 310.374659 217.282912 \r\nL 310.679023 216.210156 \r\nL 310.983386 216.880155 \r\nL 311.28775 215.876904 \r\nL 311.592114 215.46303 \r\nL 311.896477 214.484008 \r\nL 312.200841 211.643223 \r\nL 312.505205 211.816932 \r\nL 312.809568 211.819785 \r\nL 313.418295 209.881253 \r\nL 313.722659 208.583242 \r\nL 314.027023 208.031362 \r\nL 314.331386 208.253032 \r\nL 314.940114 204.321579 \r\nL 315.244477 202.973773 \r\nL 315.548841 203.354155 \r\nL 315.853205 202.811356 \r\nL 316.157568 202.616274 \r\nL 316.461932 202.59076 \r\nL 316.766295 203.550998 \r\nL 317.070659 203.376551 \r\nL 317.375023 202.965426 \r\nL 317.679386 201.814043 \r\nL 317.98375 201.192615 \r\nL 318.288114 203.27704 \r\nL 318.592477 202.957158 \r\nL 318.896841 201.999186 \r\nL 319.201205 199.061469 \r\nL 319.505568 200.62394 \r\nL 319.809932 199.290683 \r\nL 320.114295 200.350329 \r\nL 320.418659 204.542532 \r\nL 320.723023 202.692382 \r\nL 321.027386 204.27403 \r\nL 321.33175 201.796469 \r\nL 321.636114 205.878816 \r\nL 321.940477 205.922517 \r\nL 322.244841 203.575329 \r\nL 322.549205 203.67919 \r\nL 322.853568 202.268936 \r\nL 323.157932 203.905068 \r\nL 323.462295 202.830534 \r\nL 323.766659 204.200051 \r\nL 324.375386 202.933666 \r\nL 324.67975 203.31942 \r\nL 324.984114 201.439756 \r\nL 325.288477 200.785645 \r\nL 325.592841 200.914147 \r\nL 326.201568 203.299621 \r\nL 326.505932 200.870521 \r\nL 326.810295 201.465904 \r\nL 327.114659 201.190972 \r\nL 327.723386 198.180856 \r\nL 328.02775 200.111004 \r\nL 328.332114 199.878182 \r\nL 328.636477 202.286408 \r\nL 328.940841 206.504669 \r\nL 329.245205 204.459445 \r\nL 329.549568 206.661636 \r\nL 329.853932 202.80938 \r\nL 330.158295 205.650448 \r\nL 330.462659 206.01392 \r\nL 330.767023 204.021894 \r\nL 331.071386 205.560718 \r\nL 331.37575 206.36043 \r\nL 331.680114 205.973791 \r\nL 331.984477 202.923977 \r\nL 332.288841 202.699299 \r\nL 332.593205 205.290783 \r\nL 332.897568 204.261781 \r\nL 333.506295 205.832414 \r\nL 333.810659 205.889658 \r\nL 334.115023 209.021883 \r\nL 334.419386 210.789897 \r\nL 335.028114 210.399846 \r\nL 335.332477 211.151563 \r\nL 335.941205 210.478358 \r\nL 336.245568 212.679976 \r\nL 336.549932 209.990974 \r\nL 336.854295 209.046545 \r\nL 337.158659 208.955277 \r\nL 337.463023 205.946431 \r\nL 337.767386 209.857295 \r\nL 338.07175 208.186938 \r\nL 338.376114 207.474614 \r\nL 338.680477 206.244643 \r\nL 339.289205 208.954327 \r\nL 339.593568 206.757 \r\nL 339.897932 206.685616 \r\nL 340.202295 206.331482 \r\nL 340.506659 204.699699 \r\nL 340.811023 205.756545 \r\nL 341.115386 206.39803 \r\nL 341.41975 204.118274 \r\nL 341.724114 206.377183 \r\nL 342.028477 205.040343 \r\nL 342.637205 203.445412 \r\nL 342.941568 204.750225 \r\nL 343.245932 208.764311 \r\nL 343.550295 205.448102 \r\nL 343.854659 206.104461 \r\nL 344.463386 203.327011 \r\nL 344.76775 206.33199 \r\nL 345.072114 204.721984 \r\nL 345.376477 205.097494 \r\nL 345.985205 201.724511 \r\nL 346.289568 205.556781 \r\nL 346.593932 205.388413 \r\nL 346.898295 202.964749 \r\nL 347.202659 205.185171 \r\nL 347.507023 208.265395 \r\nL 347.811386 204.30015 \r\nL 348.11575 203.919455 \r\nL 348.420114 206.330848 \r\nL 348.724477 205.997093 \r\nL 349.028841 206.121505 \r\nL 349.333205 205.242738 \r\nL 349.637568 207.581803 \r\nL 349.941932 203.706508 \r\nL 350.246295 205.674842 \r\nL 350.550659 210.638225 \r\nL 350.855023 212.491862 \r\nL 351.159386 210.727532 \r\nL 351.46375 213.212257 \r\nL 351.768114 213.679769 \r\nL 352.072477 212.027912 \r\nL 352.376841 211.240234 \r\nL 352.681205 211.778712 \r\nL 352.985568 209.699706 \r\nL 353.289932 211.8978 \r\nL 353.594295 209.67902 \r\nL 353.898659 209.851399 \r\nL 354.203023 208.090295 \r\nL 354.507386 209.50629 \r\nL 354.81175 209.341188 \r\nL 355.116114 207.396111 \r\nL 355.420477 206.614652 \r\nL 355.724841 206.873872 \r\nL 356.029205 206.064357 \r\nL 356.333568 206.777784 \r\nL 356.637932 205.260068 \r\nL 356.942295 207.054208 \r\nL 357.246659 205.846862 \r\nL 357.551023 207.751912 \r\nL 357.855386 203.755967 \r\nL 358.15975 208.075296 \r\nL 358.464114 206.500429 \r\nL 358.768477 206.91609 \r\nL 359.072841 206.152634 \r\nL 359.377205 211.324974 \r\nL 359.985932 210.231737 \r\nL 360.290295 210.108658 \r\nL 360.594659 213.87076 \r\nL 360.899023 211.248322 \r\nL 361.203386 212.95579 \r\nL 361.50775 212.797478 \r\nL 361.812114 209.432256 \r\nL 362.116477 212.192166 \r\nL 362.420841 212.519639 \r\nL 362.725205 209.448582 \r\nL 363.029568 211.320982 \r\nL 363.333932 211.594221 \r\nL 363.638295 208.93545 \r\nL 363.942659 211.141621 \r\nL 364.247023 209.126719 \r\nL 364.551386 211.798721 \r\nL 365.160114 210.499148 \r\nL 365.464477 210.047525 \r\nL 365.768841 209.054049 \r\nL 366.073205 208.788796 \r\nL 366.377568 209.926137 \r\nL 366.681932 210.222002 \r\nL 366.986295 210.998477 \r\nL 367.290659 210.902257 \r\nL 367.595023 212.555389 \r\nL 367.899386 209.186495 \r\nL 368.20375 208.310525 \r\nL 368.508114 209.038867 \r\nL 368.812477 209.211577 \r\nL 369.116841 212.160097 \r\nL 369.421205 207.905695 \r\nL 369.725568 210.403323 \r\nL 369.725568 210.403323 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 50.14375 239.758125 \r\nL 50.14375 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 384.94375 239.758125 \r\nL 384.94375 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 50.14375 239.758125 \r\nL 384.94375 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 50.14375 22.318125 \r\nL 384.94375 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- Wasserstein Loss -->\r\n    <g transform=\"translate(166.200625 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 3.328125 72.90625 \r\nL 13.28125 72.90625 \r\nL 28.609375 11.28125 \r\nL 43.890625 72.90625 \r\nL 54.984375 72.90625 \r\nL 70.3125 11.28125 \r\nL 85.59375 72.90625 \r\nL 95.609375 72.90625 \r\nL 77.296875 0 \r\nL 64.890625 0 \r\nL 49.515625 63.28125 \r\nL 33.984375 0 \r\nL 21.578125 0 \r\nz\r\n\" id=\"DejaVuSans-87\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-87\"/>\r\n     <use x=\"92.501953\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"153.78125\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"205.880859\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"257.980469\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"319.503906\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"360.617188\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"412.716797\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"451.925781\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"513.449219\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"541.232422\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"604.611328\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"636.398438\" xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"690.361328\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"751.542969\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"803.642578\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p7cb26e8063\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "FS = 10\n",
    "fig = plt.figure()\n",
    "plt.rcParams[\"font.size\"] = FS\n",
    "plt.plot(DMM_dics[\"losses\"])\n",
    "plt.title(\"Wasserstein Loss\")\n",
    "plt.xlabel(\"epoch\", fontsize=FS)\n",
    "plt.ylabel(\"loss\", fontsize=FS)\n",
    "fig.savefig(\"test.png\")\n",
    "# plt.xlim(500,2000)\n",
    "# plt.ylim(0,0.06)\n",
    "# DMM_dics[\"losses\"][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.0352, requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 751
    }
   ],
   "source": [
    "DMM_dics[\"losses\"][999]"
   ]
  },
  {
   "source": [
    "## args"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(annealing_epochs=1000, beta1=0.96, beta2=0.999, checkpoint_freq=1000, clip_norm=10.0, cuda=False, iaf_dim=100, jit=False, learning_rate=5e-05, load_model='', load_opt='', log='dmm.log', lr_decay=0.99996, mini_batch_size=20, minimum_annealing_factor=0.2, num_epochs=10000, num_iafs=0, rnn_clamp=0.01, rnn_dropout_rate=0.1, rpt=True, rpt_eps=1e-20, save_model='', save_opt='', tmc=False, tmc_num_samples=10, tmcelbo=False, w_loops=10, wass_learning_rate=1e-05, weight_decay=2.0)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "DMM_dics[\"args\"]"
   ]
  },
  {
   "source": [
    "# Save as MP3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 指定されたファイルが見つかりません。",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d49ae8e9f182>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFluidSynth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 入力するmidiファイルとアウトプットファイル\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmidi_to_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TEST.mid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample.wav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\midi2audio.py\u001b[0m in \u001b[0;36mmidi_to_audio\u001b[1;34m(self, midi_file, audio_file)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmidi_to_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fluidsynth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-ni'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msound_font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-F'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplay_midi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-l\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \"\"\"\n\u001b[1;32m--> 340\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    852\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1305\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 指定されたファイルが見つかりません。"
     ]
    }
   ],
   "source": [
    "# サウンドフォントを指定する\n",
    "# fs = FluidSynth(sound_font='font.sf2')\n",
    "fs = FluidSynth()\n",
    "# 入力するmidiファイルとアウトプットファイル\n",
    "fs.midi_to_audio(\"TEST.mid\", 'sample.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}